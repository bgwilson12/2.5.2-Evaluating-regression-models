{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('..\\\\ny_crime_13.xls', header=4)\n",
    "df.head(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.DataFrame()\n",
    "df_features['pop'] = df['Population']\n",
    "df_features['pop_squared'] = df_features['pop']**2\n",
    "df_features['murder_bin'] = np.where(df.iloc[:, 3] > 0, 1, 0)\n",
    "df_features['murder'] = df.iloc[:,3]\n",
    "df_features['robbery_bin'] = np.where(df.iloc[:, 6] > 0, 1, 0)\n",
    "df_features['robbery'] = df.iloc[:, 6]\n",
    "df_features['theft_bin'] = np.where(df['Larceny-\\ntheft'] > 0, 1, 0)\n",
    "df_features['theft'] = df['Larceny-\\ntheft']\n",
    "df_features['prop_crime'] = df.loc[:, 'Property\\ncrime']\n",
    "df_features['violent'] = df['Violent\\ncrime']\n",
    "df_features.dropna(inplace=True)\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restructure the data so we can use facetgrid\n",
    "df_long = df_features\n",
    "df_long = pd.melt(df_long.drop(['murder', 'robbery', 'theft', 'theft_bin'], axis=1),\n",
    "                  id_vars=['murder_bin', 'robbery_bin'])\n",
    "g = sns.FacetGrid(df_long, col='variable', size = 5, aspect=.7, sharey=False)\n",
    "g = g.map(sns.boxplot, 'murder_bin', 'value', showfliers=False)\n",
    "plt.show()\n",
    "\n",
    "g2 = sns.FacetGrid(df_long, col='variable', size = 5, aspect=.7, sharey=False)\n",
    "g2 = g2.map(sns.boxplot, 'robbery_bin', 'value', showfliers=False)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These boxplots show that places that have murder and robbery occur more in places with higher population. The last column of boxplots shows that property crime is higher in places with robbery and murder, but that is most likely becasue areas of high crime are grouped together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "Y = df_features['prop_crime'].values.reshape(-1,1)\n",
    "parameters = ['theft', 'robbery', 'murder', 'pop', 'pop_squared']\n",
    "X = df_features[parameters]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "regr.fit(X_train,y_train)\n",
    "\n",
    "# Inspect the results.\n",
    "print('\\nCoefficients: \\n', regr.coef_)\n",
    "print('\\nIntercept: \\n', regr.intercept_)\n",
    "print('\\nR-squared on the training set:')\n",
    "print(regr.score(X_train, y_train))\n",
    "print('\\nR-squared on the test set:')\n",
    "print(regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are pretty high r-squared values, but since they are both pretty high, I'd say that we are ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_pred = regr.predict(X_test)\n",
    "residual = y_test - y_pred\n",
    "plt.hist(residual, bins=40)\n",
    "plt.title('Residual counts')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Count')\n",
    "# plt.xlim([-500, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I still do not have enough experience to know if this is considered normal or not... It almost appears that it has a left tail, and a strange bump thing on the right. Could this mean that a variable is missing, and could explain the two little bumps on either side?\n",
    "\n",
    "Or perhaps these are just outliers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "display(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using binary categorical variables, they are lowly correlated with each other. However, as we saw in the box plots, the number of crimes committed is highly correlated with the population. Both methods give a pretty reasonably good classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(y_pred, residual)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted')\n",
    "# plt.xlim([-200, 1000])\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesting. I have played around with switching the features between binary and actual values. Using the raw values produces a better classifier with a tighter cluster of residuals at lower estimates. Our classifier seems to fail at the higher predictions, and I think this may be due to the fact that we have much less data for higher populations. Perhaps we could classify these predictions as outliers, and get rid of the data that caused them? If that's the case, then we would need to be careful to only use our classifier with data that fits into the same domain as the predictions we kept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the Classifier\n",
    "_From here on, everything will be new content. All of the above was copied from a previous assignment._\n",
    "\n",
    "The first thing we should do is run a whole model F-test. I will do this by hand for my understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssef = ((df_features.prop_crime.values - y_pred)**2).sum()\n",
    "sser = ((df_features.prop_crime - df_features.prop_crime.mean())**2).sum()\n",
    "n = len(df_features.prop_crime)\n",
    "pf = len(parameters)\n",
    "pr = 1\n",
    "dff = n - pf\n",
    "dfr = n - pr\n",
    "\n",
    "f_test = ((ssef-sser)/(dff - dfr))/(ssef/dff)\n",
    "f_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is unclear at this point what I should do with a p-value of $-84.5$. I am fairly sure that this means my model is significant, but then again I'm unsure because I've never seen a negative p-value, and the f-test distribution itself is never negative either. So... Good to go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_formula = 'prop_crime ~ theft+robbery+murder+pop+pop_squared'\n",
    "\n",
    "# Now fit the model using a different module then before so we can pull out the\n",
    "# individual p-values of significance for each parameter\n",
    "lin_mod = smf.ols(formula=linear_formula, data=df_features).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_mod.pvalues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add another feature, but we will add it to a separate model. I want to see how an additional feature will affect the generalization to another dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr2 = linear_model.LinearRegression()\n",
    "Y = df_features['prop_crime'].values.reshape(-1,1)\n",
    "parameters = ['theft', 'robbery', 'murder', 'pop', 'pop_squared', 'violent']\n",
    "X = df_features[parameters]\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.2, random_state=42)\n",
    "regr2.fit(X_train,y_train)\n",
    "\n",
    "# Inspect the results.\n",
    "print('\\nCoefficients: \\n', regr2.coef_)\n",
    "print('\\nIntercept: \\n', regr2.intercept_)\n",
    "print('\\nR-squared on the training set:')\n",
    "print(regr2.score(X_train, y_train))\n",
    "print('\\nR-squared on the test set:')\n",
    "print(regr2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr2.predict(X_test)\n",
    "residual = y_test - y_pred\n",
    "plt.hist(residual, bins=40)\n",
    "plt.title('Residual counts')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Count')\n",
    "# plt.xlim([-500, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "display(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From my discussion with Katherine, high multi-collinearity results in an unstable model. This manifests itself in coefficients that are different every time we run the model. However, I'm operating on the technicality that the assignment originally asked for a model that explains the most variance possible, rather than understanding the mechanisms. So, with that in mind, I'm going to move on even though these are all really highly correlated!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(y_pred, residual)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted')\n",
    "# plt.xlim([-200, 1000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This got better as well, and I'm pretty pleased with that. I have a gut feeling that we are overfitting, becasue that r-value is extremely high. Let's find out with a new data set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load in a brand new dataset. I chose the california crime dataset, also from 2013.\n",
    "df_ca = pd.read_excel('..\\\\ny_crime_13.xls', header=4)\n",
    "df_ca.head(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the same features that we made for the NY dataset\n",
    "df_ca_features = pd.DataFrame()\n",
    "df_ca_features['pop'] = df['Population']\n",
    "df_ca_features['pop_squared'] = df_ca_features['pop']**2\n",
    "df_ca_features['murder'] = df.iloc[:,3]\n",
    "df_ca_features['robbery'] = df.iloc[:, 6]\n",
    "df_ca_features['theft'] = df['Larceny-\\ntheft']\n",
    "df_ca_features['prop_crime'] = df.loc[:, 'Property\\ncrime']\n",
    "df_ca_features['violent'] = df['Violent\\ncrime']\n",
    "df_ca_features.dropna(inplace=True)\n",
    "df_ca_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will treat the entire dataset as a test set, because we want to see\n",
    "# how our model generalizes to a different dataset.\n",
    "Y_ca = df_ca_features['prop_crime'].values.reshape(-1,1)\n",
    "parameters = ['theft', 'robbery', 'murder', 'pop', 'pop_squared', 'violent']\n",
    "X_ca = df_ca_features[parameters]\n",
    "\n",
    "print('\\nR-squared on the test set:')\n",
    "print(regr2.score(X_ca, Y_ca))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow. This model still has an extremely high $R^2$ value. I'm still really hesitant to say that we are doing well, but then again maybe this dataset was chosen specifically for its ease of model fitting. I just can't say at this point!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regr2.predict(X_ca)\n",
    "residual = Y_ca - y_pred\n",
    "plt.hist(residual, bins=40)\n",
    "plt.title('Residual counts')\n",
    "plt.xlabel('Residual')\n",
    "plt.ylabel('Count')\n",
    "# plt.xlim([-500, 500])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.scatter(y_pred, residual)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.axhline(y=0)\n",
    "plt.title('Residual vs. Predicted')\n",
    "plt.xlim([-200, 13000])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to be fine too, I think. Turning it in."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
